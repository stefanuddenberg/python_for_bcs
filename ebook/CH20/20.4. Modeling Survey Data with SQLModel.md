# 20.4. Modeling Survey Data with SQLModel

In our web survey, participants will submit their responses through a form in their browser. On the
server side, we need a way to do three things:

1. **Validate** incoming data (e.g., make sure a 1-7 Likert rating is actually an integer between 1 and 7, not a string or a float)
2. **Store** that data in a database, so we can analyze it later
3. **Export** that data later, so we can analyze it and share it with other researchers

Luckily, we have SQLModel to help us with all of these things.

SQLModel is built on top of:

- **SQLAlchemy** — the most popular Object-Relational Mapping (ORM) library in Python
- **Pydantic** — a library for data validation using Python type hints (recall that we covered type hints briefly in [Chapter 6.4 Data Classes](../CH06/6.4.%20Data%20Classes.md))

Pydantic classes look a lot like data classes, but they come with a number of extra features. Most
importantly for our purposes, they come with automatic data validation, and they can be used to
create our database tables as well.

## Step 1: Create a file called `models.py`

In your root project folder (where the `main.py` script lives), create a file called `models.py`.
Let's start by adding a base class for our survey responses. This will store all the shared fields
for our various types of survey response classes.

```python
from datetime import datetime, timezone

from sqlmodel import Field, SQLModel


class SurveyResponseBase(SQLModel):
    """
    The shared fields for a survey response.
    """

    # TIPI-style items (intentionally minimal; no PII).
    # Scale: 1 = disagree strongly, 7 = agree strongly.
    tipi_extraverted: int = Field(ge=1, le=7)
    tipi_reserved_quiet: int = Field(ge=1, le=7)

    # Optional open-ended response.
    comments: str | None = Field(default=None, max_length=2000)
```

Next, let's create the table that will store our survey responses. This table will inherit from our
base class, and we'll set `table=True` to let SQLModel know that we will create an SQL table for
this class. It's important to note that in a database, each row is a unique entry, so it needs an
`id` field to uniquely identify each row. We'll also add a `primary_key=True` flag to the `id` field
to tell SQLModel that this is the primary key for the table. In addition, it helps to know when each
response was created, so we will add a `created_at` field to the table, such that each response has
a timestamp (in UTC time) of when it was created.

```python
class SurveyResponse(SurveyResponseBase, table=True):
    """
    This is the database table that will store our survey responses.

    Note the use of `table=True`; this will allow SQLModel to create an SQL table.
    """

    id: int | None = Field(default=None, primary_key=True)
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
```

Next, let's make a helper class for sending formatted survey response data from the server to the
client.

```python

class SurveyResponseOut(SurveyResponseBase):
    """
    This is what we send back to the client (it includes server-generated fields).

    This output model is used to ensure that we don't accidentally return private information
    back to the client (in this case, the `id` field).

    In a real-world application, we might also have an "input model" for when the client
    sends data to the server as well, to validate that data before trying to save it.
    """

    created_at: datetime
```

## Interim summary

### Reusable data models

Having done all that, we've set up a number of useful data models, one of which is simultaneously a database table. For example:

- `SurveyResponseBase` is _not_ a database table. It is just a reusable set of typed fields, representing the shared fields for all survey response data models we will use.
- `SurveyResponse` **is** a database table because it includes `table=True` in its class definition.
- `SurveyResponseOut` is _not_ a database table. It is used to send the formatted survey response
  back to the client.

This pattern keeps things clean: we can reuse the same field definitions for both "API input/output"
and "database storage", but still keep the database-only fields (like `id`) private and separate.

### `Field(...)` lets you add database and validation rules

We made use of the `Field(...)` function for each of the properties of the classes. This is a special function that allows us to add database and validation rules to our data models. For example:

- `id: int | None = Field(default=None, primary_key=True)` creates an auto-incrementing integer primary key.
- `tipi_extraverted: int = Field(..., ge=1, le=7)` validates that a Likert rating is 1-7. `ge` means "greater than or equal to" and `le` means "less than or equal to".
- `created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))` creates a
  timestamp in UTC time (to avoid confusion surrounding what timezone the participant was in). Note
  the use of the `default_factory` parameter to set the default value to the current UTC time at the
  time of creation! This is different from other types of default values, as it is a dynamic value
  (differing with each new instance of the class) rather than a static one.
- `comments: str | None = Field(default=None, max_length=2000)` validates that the comments are an
  optional string with a maximum length of 2000 characters.

Of course, there are many other types we could use in our database. We could use `float` for
computed trait scores, or `bool` for yes/no responses, and so on. Often, for these computed values,
they need not be validated as `Field(...)` because they are computed after the fact. So in that case
you can just declare them as you ordinarily would in a data class. E.g., `extraversion: float` or
`submitted_comments: bool`.

Having gone to all this effort, later on, when the client submits data via FastAPI, invalid values
will automatically produce helpful error messages.

## Next steps

This section was all about setting up the architecture for our database and data models. But on
their own, they're quite useless. In the next section we will need to put them to use by:

- creating a database engine
- creating the database tables automatically
- opening a database session and inserting rows into the database
- exporting the data from the database in CSV format

<!-- GITHUB-NAV-START -->

Next: [20.5. Setting Up the Database](20.5.%20Setting%20Up%20the%20Database.md)<br>
Previous: [20.3. Drafting the Survey with HTML](20.3.%20Drafting%20the%20Survey%20with%20HTML.md)

<!-- GITHUB-NAV-END -->
